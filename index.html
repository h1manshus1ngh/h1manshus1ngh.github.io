<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Himanshu Singh</title>
  
  <meta name="author" content="Himanshu Singh">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>üåê</text></svg>">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody> 
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Himanshu Singh</name>
              </p>
              <p>I am a Ph.D. Research Scholar being advised by <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/faculty-list/~badrinsubudhi/">Dr. Badri N Subudhi</a> and <a target="_blank" href="https://www.iitjammu.ac.in/faculty/~vinitjakhetiya/">Dr. Vinit Jakhetiya</a> in the EE and CSE Department at <a target="_blank" href="https://www.iitjammu.ac.in/">Indian Institute of Technology Jammu, India</a> since 2021. I primarily work in the area of image processing, computer vision and action recognition and localization in complex videos. 
              </p>
              <p>
                I have completed my M.E. in Signal Processing from <a target="_blank" href="https://https://www.iisc.ac.in/">Indian Institute of Science</a>(A Institute of Eminence), Bangalore, India under the supervision of <a target="_blank" href="https://www.ee.iisc.ac.in/ramakrishnan-a-g/">Professor.A G Ramakrishnan 
</a>(Professor, IISc, Bangalore) in 2013. I have also worked as a Assitant Professor at <a target="_blank" href="https://www.nitgoa.ac.in/"> 
		      National Institute of Technology Goa, India. 



              </p>
              <p style="text-align:center">
                <a target="_blank" href="mailto:2020ree2057@iitjammu.ac.in">Email</a> &nbsp/&nbsp
                <a target="_blank" href="https://scholar.google.com/citations?user=JZtGsbwAAAAJ&hl=en">Google Scholar</a> &nbsp/&nbsp
                <a target="_blank" href="https://www.linkedin.com/in/himanshu-singh-53b1941a0/">Linkedin</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/dd.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/dd.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
           
	 <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>News</heading>
		 <p> 25/10/2024- One paper has been accepted in INDICON 2024.</p>
		 <p> 17/08/2024- One paper has been accepted in ICPR 2024.</p>
 		 <p> 02/11/2022- One paper has been accepted in IEEE TAI-2022.</p>
		 <p> 02/02/2021- Awarded AAAI student scholarship of 250 USD from Association for the Advancement of Artificial Intelligence.</p>          

            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
		<tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                I'm interested in Action recognition and localization in normal light condtion as well as dark light videos. Representative papers are highlighted. 
              </p>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		<!------------------------------Paper 4--------------------------------->	
    <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img style="width: 160px;" src='images/4.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Self-Attention based Architecture for Forecasting the Pandemic Spread in India</papertitle>
              </a>
              <be>
	 <a target="_blank" href="https://www.linkedin.com/in/meghanshu-v-855713246/">, Meghanshu Verma </a>, 	
         <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/post/phd-students-electrical">Himanshu Singh</a>, 
	 <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/faculty-list/~badrinsubudhi">Badri N Subudhi</a>,
	 <a target="_blank" href="https://iitjammu.ac.in/computer_science_engineering/faculty-list/~vinitjakhetiya">Vinit Jakhetiya</a>,
              <em></sup>in Proceedings of the IEEE India Council International Conference (INDICON), Kharagpur, India</em>, 2024.
              <br>
              <p></p>
              <p>
              In this paper, we propose an intelligent framework for forecasting infectious disease trajectories using a self-attention mechanism and visualizing results at the district level on India's map. The framework employs feature engineering, time-series conversion, and a self-attention module to capture complex spatial-temporal patterns for reliable predictions. Evaluated on the COVID-19 India dataset, the model achieves a mean absolute percentage error (MAPE) of 87.73, outperforming seven state-of-the-art methods, with district-level predictions visualized through geo-mapping.</p>
            </td>
          </tr>
	<heading>Publications</heading>	
		<!------------------------------Paper 3--------------------------------->	
			
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img style="width: 160px;" src='images/3.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
                <papertitle>Project and Pool: An Action Localization network for localizing actions in Untrimmed Videos</papertitle>
              </a>
              <br>
         <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/post/phd-students-electrical">Himanshu Singh</a>, 
        <a target="_blank" href="https://www.linkedin.com/in/avijit-dey-188a351a8/?originalSubdomain=in">, Avijit Dey </a>, 
	<a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/faculty-list/~badrinsubudhi">Badri N Subudhi</a>,
	<a target="_blank" href="https://iitjammu.ac.in/computer_science_engineering/faculty-list/~vinitjakhetiya">Vinit Jakhetiya</a>,
              <em></sup>in Proceedings of the International Conference on Pattern Recognition (ICPR), Kolkata, India</em>, 2024.
              <br>
              <p></p>
              <p>
               In this paper, the proposed algorithm follows a three-stage framework for action recognition and localization. In the first stage, spatial and temporal features are projected using a two-layer LSTM module called LSTMProjector, which captures both local and global dependencies in the input video sequences. In the second stage, a latent space projection is performed using a one-dimensional convolutional layer to harmonize the dimensions of spatial and temporal features. In the final stage, a parameter-free temporal pooling block selectively extracts critical information from local clip embeddings, improving the model‚Äôs efficiency in action localization.</p>
            </td>
          </tr>
          
		<!------------------------------Paper 2--------------------------------->
		
         <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img style="width: 160px;" src='images/1.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9954149">
                <papertitle>Action Recognition in Dark Videos using Spatio-temporal Features and Bidirectional Encoder Representations from Transformers</papertitle>
              </a>
              <br>
         <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/post/phd-students-electrical">Himanshu Singh</a>, 
        <a target="_blank" href="https://in.linkedin.com/in/saurabh-suman-a83734199">Saurabh Suman</a>, 
	<a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/faculty-list/~badrinsubudhi">Badri N Subudhi</a>,
	<a target="_blank" href="https://iitjammu.ac.in/computer_science_engineering/faculty-list/~vinitjakhetiya">Vinit Jakhetiya</a>,
        <a target="_blank" href="https://www.isical.ac.in/~ash/">Ashish Ghosh</a>,
              <em></sup>in IEEE Transactions on Artificial Intelligence</em>, 2022.
              <br>
              <a target="_blank" href="https://ieeexplore.ieee.org/abstract/document/9954149">Paper Link</a>
              <p></p>
              <p>
               In this paper, The proposed algorithm follows two-stages for action recognition. In the first stage, the low-light videos are enhanced using Zero-Reference Deep Curve Estimation (Zero-DCE), followed by the min-max sampling algorithm. In the latter stage, we propose an action classification network to recognize the actions in the enhanced videos.</p>
            </td>
          </tr>
          <!---------------------------Paper 1------------------------------------>
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <img style="width: 160px;" src='images/2.png'>
              </div>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/21662">
                <papertitle>C3D and Localization Model for Locating and Recognizing the Actions from Untrimmed Videos (Student Abstract)</papertitle>
              </a>
              <br>
        <a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/post/phd-students-electrical">Himanshu Singh</a>, 
        <a target="_blank" href="https://in.linkedin.com/in/tirupati-pallewad-047b69158">Tirupati Pallewad</a>, 
	<a target="_blank" href="https://iitjammu.ac.in/electrical_engineering/faculty-list/~badrinsubudhi">Badri N Subudhi</a>,
	<a target="_blank" href="https://iitjammu.ac.in/computer_science_engineering/faculty-list/~vinitjakhetiya">Vinit Jakhetiya</a>

        
              <em>in Proceedings of the AAAI Conference on Artificial Intelligence, Vancouver Canada</em>, 2021
              <br>
              <a target="_blank" href="https://ojs.aaai.org/index.php/AAAI/article/view/21662">Paper Link</a>
              <p></p>
	      <p>
		In this paper,  we proposed a technique for action localization and recognition from long untrimmed videos. It consists of C3D CNN model followed by the action mining using the localization model, where the KNN classifier is used. We segment the video into expressible sub-action known as action-bytes. The pseudo labels have been used to train the localization model, which makes the trimmed videos untrimmed for action-bytes. We present experimental results on the recent benchmark trimmed video dataset ‚ÄúThumos14‚Äù.</p>
	</td>
        </tr>



          </tr>				
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p style="text-align:right;font-size:small;">
                Thanks for Visiting my Page.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <p style="text-align:right;font-size:small;">
                <!-- hitwebcounter Code START -->
<a href="https://www.hitwebcounter.com" target="_blank">
<img src="https://hitwebcounter.com/counter/counter.php?page=8120766&style=0025&nbdigits=5&type=ip&initCount=50" title="Free Counter" Alt="web counter"   border="0" /></a>   
              </p>
            </td>
          </tr>
        </tbody></table>
</body>

</html>
